üìå Deploying, Training, and Running a Pretrained Model for SQL & Database Research in GGUF Format
Project Objectives:

‚úÖ Identify a Pretrained Model for SQL and Database Research
‚úÖ Set Up InstructLab on an NVIDIA GPU-Powered System
‚úÖ Download and Prepare the Model for Use
‚úÖ Train the Model with Custom SQL Data
‚úÖ Convert and Deploy the Model in GGUF Format
‚úÖ Execute and Assess Model Performance

üöÄ SQLCoder API - CUDA-Optimized Deployment

This repository provides an SQLCoder API powered by FastAPI, optimized for CUDA acceleration on NVIDIA GPUs. It supports SQL-based NLP tasks using the ollama and vllm frameworks, leveraging PyTorch and Hugging Face transformers.

üîß Setup Instructions

1Ô∏è‚É£ Install Dependencies

Ensure the following components are installed:

‚úÖ Docker (latest version)

‚úÖ NVIDIA Drivers (for GPU acceleration)

‚úÖ NVIDIA Container Toolkit (for GPU-enabled Docker)

‚úÖ At least 16GB RAM (to ensure smooth execution)

To confirm NVIDIA drivers are correctly installed, run:

nvidia-smi

If successful, your GPU details should be displayed.

2Ô∏è‚É£ Build & Launch the Docker Container

Build the Image:

docker-compose build --no-cache

Start the Container:

docker-compose up -d

3Ô∏è‚É£ Validate the Model & API

Test FastAPI Server:

üìå API Documentation: http://127.0.0.1:8000/docs

üìå OpenAPI JSON Spec: http://127.0.0.1:8000/openapi.json

Verify CUDA is Enabled Inside the Container:

docker exec -it sqlcoder python3 -c "import torch; print(torch.cuda.is_available())"

‚úÖ If True, CUDA is correctly set up.

‚ùå If False, check the NVIDIA driver and CUDA installation.

üß† Manually Installing & Training SQLCoder Inside the Container

As downloading and training large models can be time-consuming, it‚Äôs best to install them inside the running container.

1Ô∏è‚É£ Access the Running Container

docker exec -it sqlcoder /bin/bash

2Ô∏è‚É£ Download SQLCoder-7B Model

mkdir -p /root/.local/share/instructlab/checkpoints
wget -O /root/.local/share/instructlab/checkpoints/sqlcoder-trained-Q4_K_M.gguf \
https://example.com/path-to-your-model/sqlcoder-trained-Q4_K_M.gguf

This ensures the model is properly downloaded and avoids build-time failures.

üöÄ Using SQLCoder Inside the Container

1Ô∏è‚É£ Test the Model in a Chat Session

docker exec -it sqlcoder ilab model chat \
    -m /root/.local/share/instructlab/checkpoints/sqlcoder-trained-Q4_K_M.gguf

2Ô∏è‚É£ Evaluate Model Performance

docker exec -it sqlcoder ilab model evaluate --benchmark mmlu_branch \
    --model /root/.local/share/instructlab/checkpoints/sqlcoder-trained-Q4_K_M.gguf \
    --base-model ~/.cache/instructlab/models/sqlcoder-7b.gguf

üõ† Ensuring Persistent Model Storage

To retain SQLCoder models across container restarts, modify docker-compose.yml to include:

volumes:
  - ./models:/root/.local/share/instructlab/checkpoints:rw

This prevents model files from being deleted when the container stops.

üìú Optimizations & Cleanup

1Ô∏è‚É£ Remove All Docker Data (Optional)

To clean up old Docker images and volumes:

docker system prune -a --volumes

2Ô∏è‚É£ Rebuild the Image

docker-compose build --no-cache

3Ô∏è‚É£ Restart the Container

docker-compose up -d

üöÄ This should result in a significantly faster build process.

üìå Training SQLCoder on Custom SQL Datasets

1Ô∏è‚É£ Clone the InstructLab Taxonomy Repository

git clone --depth=1 https://github.com/instructlab/taxonomy.git

cd taxonomy

2Ô∏è‚É£ Create a Custom Training Dataset

mkdir -p knowledge/computer_science/databases/sql/qna

echo -e "question: 'What is an index in a database?'\nanswer: 'An index improves query performance by allowing faster lookups.'" \

> knowledge/computer_science/databases/sql/qna.yaml

3Ô∏è‚É£ Train SQLCoder on Custom Data

# Activate virtual environment
source /opt/venv/bin/activate

# Start training
ilab model train --pipeline accelerated --device cuda --data-path ~/.local/share/instructlab/datasets/

This fine-tunes SQLCoder using GPU acceleration.

üíæ Saving the Trained Model to a New Docker Image

To persist your trained model and dependencies:

docker commit sqlcoder my-sqlcoder-image

docker tag my-sqlcoder-image my-repo/sqlcoder:latest

Now, models won‚Äôt need to be redownloaded on each container start.

Converting & Deploying the Model in GGUF Format

After training SQLCoder with custom SQL datasets, converting it to GGUF format ensures optimized deployment.

1Ô∏è‚É£ Convert Model to GGUF Format

Inside the running container, execute:

docker exec -it sqlcoder bash
ilab model convert --input /root/.local/share/instructlab/checkpoints/sqlcoder-trained.bin \
                    --output /root/.local/share/instructlab/checkpoints/sqlcoder-trained-Q4_K_M.gguf \
                    --format gguf
This process converts the model to GGUF, improving inference efficiency.

2Ô∏è‚É£ Verify Model Conversion

Check if the .gguf file is present:

ls -lh /root/.local/share/instructlab/checkpoints/

3Ô∏è‚É£ Serve the GGUF Model for Inference
To start the API service inside the container:

uvicorn app.main:app --host 0.0.0.0 --port 8000 --workers 4

üöÄ Running & Evaluating the Model

1Ô∏è‚É£ Start a Chat Session with the Trained Model

docker exec -it sqlcoder ilab model chat \
    -m /root/.local/share/instructlab/checkpoints/sqlcoder-trained-Q4_K_M.gguf

2Ô∏è‚É£ Benchmark the Model Performance

Evaluate performance on the MMLU dataset:

docker exec -it sqlcoder ilab model evaluate --benchmark mmlu_branch \
    --model /root/.local/share/instructlab/checkpoints/sqlcoder-trained-Q4_K_M.gguf \
    --base-model ~/.cache/instructlab/models/sqlcoder-7b.gguf

3Ô∏è‚É£ Review Performance Metrics

After running the evaluation, analyze accuracy, latency, and execution speed.

üîó Useful Resources

üîπ SQLCoder Repository: [GitHub](https://github.com/defog-ai/sqlcoder)

üîπ FastAPI Documentation: [FastAPI Docs](https://fastapi.tiangolo.com/)

üîπ NVIDIA Container Toolkit: Setup Guide https://docs.nvidia.com/datacenter/cloud-native/container-toolkit/latest/install-guide.html


